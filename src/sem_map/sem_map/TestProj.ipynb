{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rclpy.node import Node\n",
    "from sensor_msgs.msg import Image\n",
    "from cv_bridge import CvBridge\n",
    "import cv2\n",
    "from PIL import Image as PILImage, PngImagePlugin\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import rclpy\n",
    "import rclpy.logging\n",
    "from rclpy.executors import MultiThreadedExecutor\n",
    "import cv2\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "rclpy.init(args=None)\n",
    "image_subscriber = ImageSubscriber()\n",
    "\n",
    "from utils import *\n",
    "rscalc = RealSensePointCalculator()\n",
    "\n",
    "executor = MultiThreadedExecutor()\n",
    "executor.add_node(image_subscriber)\n",
    "executor.add_node(rscalc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    while rclpy.ok():\n",
    "        print(1)\n",
    "        executor.spin_once(timeout_sec=0.1)\n",
    "        if image_subscriber.pil_image is not None:\n",
    "            print(2)\n",
    "            pil_image = image_subscriber.pil_image\n",
    "            if rscalc.info_received():\n",
    "                break\n",
    "        time.sleep(0.1)\n",
    "        print(3)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "finally:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = rscalc.get_clock().now().nanoseconds * 1e-9\n",
    "print(ct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pil_image.size)\n",
    "# display the image using matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(pil_image)\n",
    "# plt.imshow(image[0].permute(1, 2, 0).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before running this, add lang-seg folder to your python path\n",
    "from LoadLSeg import *\n",
    "import torchvision.transforms as transforms\n",
    "# from PIL import Image # use PILImage instead\n",
    "from test_lseg_zs import *\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(480),           # Resize the shorter side to 480 while maintaining aspect ratio\n",
    "    transforms.CenterCrop((480, 480)),  # Crop the center to 480x480\n",
    "    transforms.ToTensor()            # Convert to tensor\n",
    "])\n",
    "\n",
    "image = PILImage.open(\"/home/fyp/Pictures/TestSeg/bottle_keyboard.jpg\")\n",
    "image_tensor = transform(image)\n",
    "image_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(image_tensor.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "with torch.no_grad():\n",
    "    feat = model(image_tensor.unsqueeze(0).cuda())\n",
    "feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = \"keybaord\"\n",
    "threshold = 0.96\n",
    "text_feat1 = model.encode_text(search)\n",
    "sim_image = feat.half() @ text_feat1.t()\n",
    "sim_image[sim_image < sim_image.max()*threshold] = 0\n",
    "display_img2 = sim_image.detach().cpu().numpy()\n",
    "plt.imshow(display_img2, cmap='gray', vmin=display_img2.min(), vmax=display_img2.max())\n",
    "# coordinates of the maximum value in the similarity image\n",
    "print(\"###############################################\")\n",
    "print(f\"searching for: {search}\")\n",
    "print(\"###############################################\")\n",
    "x, y = float((sim_image.argmax() // sim_image.shape[1]).detach().cpu()), float((sim_image.argmax() % sim_image.shape[1]).detach().cpu())\n",
    "print(\"max value at (coordinate in pixel):\", f\"row {x}\", f\"col {y}\")\n",
    "print(\"max value at (coordinate %):\", round(x/480,2), round(y/480, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/home/fyp/llmbot2_ws/src/sem_map/sem_map\")\n",
    "from map_utils import PCA_and_Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_image = PCA_and_Cluster(feat)\n",
    "plt.imshow(clustered_image, cmap='gray', vmin=clustered_image.min(), vmax=clustered_image.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_pixels = obtain_key_pixels(feat, clustered_image)\n",
    "key_points = rscalc.obtain_key_points(key_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tf2_ros\n",
    "camera_pose = tf2_ros.TransformStamped()\n",
    "type(camera_pose.transform.rotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tf_transformations\n",
    "point = np.array([0.5, 0.5, 1])\n",
    "translation = np.array([camera_pose.transform.translation.x,\n",
    "                        camera_pose.transform.translation.y,\n",
    "                        camera_pose.transform.translation.z])\n",
    "rotation = [camera_pose.transform.rotation.x,\n",
    "            camera_pose.transform.rotation.y,\n",
    "            camera_pose.transform.rotation.z,\n",
    "            camera_pose.transform.rotation.w]\n",
    "rotation_matrix = tf_transformations.quaternion_matrix(rotation)[:3, :3]\n",
    "transformed_point = np.dot(rotation_matrix, point) + translation\n",
    "print(point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "points = [np.array([0.5, 0.5, 1]), np.array([0.5, 0.5, 1]), np.array([0.5, 0.5, 1]), np.array([0.5, 0.5, 1]), np.array([0.5, 0.5, 1]), np.array([0.5, 0.5, 1])]\n",
    "key_i = np.arange(int(len(points)*0.5))\n",
    "key_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "executor.shutdown()\n",
    "image_subscriber.destroy_node()\n",
    "rscalc.destroy_node()\n",
    "rclpy.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
